{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised Learning\n",
    "The objective of this lab project is to go further in the understanding of Self-Supervised Learning (SSL). By the end of the notebook, you will\n",
    "- Train models using different pretext tasks: colorizing, inpainting, masking reconstruction.\n",
    "- Fine-tune the models with the downstream task of interst.\n",
    "- Compare the performance of the different backbones obtained from the different downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretext and Downstream Tasks\n",
    "\n",
    "We will train three different models using three different pretext tasks. All three models will be trained on the SVHN dataset. The three models are the following:\n",
    "- A Colorization Neural Network\n",
    "- An Inpainting Neural Network\n",
    "- A Masked Autoencoder\n",
    "\n",
    "We will build a common architecture so that all three models have as similar architectures as possible. The common architecture will consist of an encoder and a decoder. Once the models pre-trained on their respective pretext tasks, we will use the pre-trained encoders to evaluate the learnt representations on two new downstream task: image classification on MNIST and on SVHN. To that end, we will perform a linear evaluation protocol, by freezing the weights of the pre-trained encoders, and training a linear classifier on the learnt representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ../data/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 182040794/182040794 [00:26<00:00, 6808211.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ../data/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 64275384/64275384 [00:11<00:00, 5526943.89it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# SVHN Dataset (Train and Test)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32)),\n",
    "])\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32)),\n",
    "])\n",
    "\n",
    "# Use SVHN train dataset for pre-training\n",
    "svhn_train = datasets.SVHN(root='../data', split='train', download=True, transform=transform)\n",
    "svhn_train_loader = DataLoader(svhn_train, batch_size=128, shuffle=True)\n",
    "\n",
    "# Use SVHN test dataset for monitoring\n",
    "svhn_test = datasets.SVHN(root='../data', split='test', download=True, transform=transform)\n",
    "svhn_test_loader = DataLoader(svhn_test, batch_size=128, shuffle=False)\n",
    "\n",
    "\"\"\" # Use SVHN extra dataset for pre-training\n",
    "svhn_extra = datasets.SVHN(root='../data', split='extra', download=True, transform=transform)\n",
    "extra_loader = DataLoader(svhn_extra, batch_size=64, shuffle=True) \"\"\"\n",
    "\n",
    "# MNIST Dataset (Fine-tuning and classification)\n",
    "mnist_train = datasets.MNIST(root='../data', train=True, download=True, transform=mnist_transform)\n",
    "mnist_test = datasets.MNIST(root='../data', train=False, download=True, transform=mnist_transform)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared architecture\n",
    "We will create a shared architecture with the following layers:\n",
    "- The encoder (in sequential order):\n",
    "    - A convolution with 64 filters, kernel size 4, stride 2, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with 128 filters, kernel size 4, stride2, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with `latent_dim`, kernel size 4, stride 2, padding 1\n",
    "    - A ReLU activation\n",
    "The encoder should take the number of channels of the input `in_channels` and the hidden dimension `latent_dim` as arguments.\n",
    "- The decoder (in secuential ) order:\n",
    "    - A Transpose convolution with 128 filters, kernel size 4, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with 16428 filters, kernel size 4, stride2, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with `out_channels` filters, kernel size 4, stride 2, padding 1\n",
    "    - A ReLU activation\n",
    "The encoder should take the number of channels of the output `out_channels` and the hidden dimension `latent_dim` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the encoder architecture\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "\n",
    "# TODO: Define the decoder architecture\n",
    "class Decoder(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/encoder_decoder.py\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, latent_dim, kernel_size=4, stride=2, padding=1),  # 4x4 -> 2x2\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, out_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=2, padding=1),  # 2x2 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl_model(model, \n",
    "                    train_loader, \n",
    "                    test_loader, \n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    device=device,\n",
    "                    epochs=5):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(images)\n",
    "            loss = criterion(output, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in test_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = model(images)\n",
    "                val_loss = criterion(output, images)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretext 1: Colorization\n",
    "\n",
    "**Questions.** \n",
    "1. What are the values of `in_channels` and `out_channels` for the Colorization model?\n",
    "2. Given the structure of the `train_ssl_model` function above, where should the conversion from the original images to the grayscale ones happen?\n",
    "3. What loss should we use for the training?\n",
    "4. What optimizer would you choose for the training?\n",
    "5. Are there other hyper-parameters that we need to choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the colorization model\n",
    "class ColorizationModel(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/colorization_model.py\n",
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, in_channels=1)  # Input grayscale\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, out_channels=3)  # Predict RGB\n",
    "\n",
    "    def forward(self, x):\n",
    "        grayscale_x = transforms.Grayscale()(x)  # Convert RGB to Grayscale\n",
    "        z = self.encoder(grayscale_x)\n",
    "        return self.decoder(z), grayscale_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colorization Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pre-train the encoder using SVHN dataset\n",
    "colorization_model = # TODO: Instantiate the colorization model\n",
    "colorization_encoder = # TODO: Pre-train the encoder using SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pretrain_colorization.py\n",
    "colorization_model = ColorizationModel(latent_dim=128)\n",
    "colorization_encoder = train_ssl_model(colorization_model, \n",
    "                                       svhn_train_loader, \n",
    "                                       svhn_test_loader, \n",
    "                                       criterion=nn.MSELoss(), \n",
    "                                       optimizer=optim.Adam(colorization_model.parameters(), lr=0.001)\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Visualize colorization on random test images\n",
    "\n",
    "def visualize_reconstructions(model, data_loader, device, num_images=5):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert the DataLoader to a list to randomly sample images\n",
    "    dataset = list(data_loader.dataset)\n",
    "\n",
    "    # Randomly select `num_images` images from the dataset\n",
    "    random_indices = random.sample(range(len(dataset)), num_images)\n",
    "    random_images = [dataset[i][0] for i in random_indices]  # Extract only the images, ignoring labels\n",
    "\n",
    "    # Stack the images into a batch\n",
    "    images = torch.stack(random_images)\n",
    "\n",
    "    # Move images to the specified device\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # Run the grayscale images through the colorization model\n",
    "    with torch.no_grad():\n",
    "        reconstructed_images, perturbed_images = model(images)\n",
    "    \n",
    "    # Move images back to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    reconstructed_images = reconstructed_images.cpu()\n",
    "    perturbed_images = perturbed_images.cpu()\n",
    "    \n",
    "    # Plot the grayscale, ground truth, and colorized images\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(10, num_images * 4))\n",
    "    for i in range(num_images):\n",
    "        # Grayscale input\n",
    "        axes[i, 0].imshow(perturbed_images[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Grayscale Input\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth (original RGB image)\n",
    "        axes[i, 1].imshow(images[i].permute(1, 2, 0))\n",
    "        axes[i, 1].set_title(\"Ground Truth (RGB)\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Colorized output from the model\n",
    "        axes[i, 2].imshow(reconstructed_images[i].permute(1, 2, 0))\n",
    "        axes[i, 2].set_title(\"Colorized Output\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize colorization on random test images\n",
    "visualize_reconstructions(colorization_model, svhn_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretext 2: Inpainting\n",
    "\n",
    "**Questions.** \n",
    "1. What are the values of `in_channels` and `out_channels` for the Inpainting model?\n",
    "2. Given the structure of the `train_ssl_model` function above, where should the masking of the original images happen?\n",
    "3. What loss should we use for the training?\n",
    "4. What optimizer would you choose for the training?\n",
    "5. Are there other hyper-parameters that we need to choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define the inpainting model by adding an apply_mask method inside the class\n",
    "class InpaintingModel(nn.Module):\n",
    "    # TODO: Implement the __init__ method\n",
    "\n",
    "    # TODO: Implement the forward method\n",
    "\n",
    "    # TODO: Implement the apply_mask method\n",
    "    def apply_mask(self, x):\n",
    "        masked_x = x.clone()\n",
    "\n",
    "        for i in range(masked_x.size(0)): # Loop over the batch size\n",
    "            ul_x = # TODO: Randomly sample the x coordinate of the upper left corner\n",
    "            ul_y = # TODO: Randomly sample the y coordinate of the upper left corner\n",
    "            # TODO: Apply the mask to the image\n",
    "\n",
    "        return masked_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/inpainting_model.py\n",
    "class InpaintingModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128, mask_size=8):\n",
    "        super(InpaintingModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        self.mask_size = mask_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = self.apply_mask(x)\n",
    "        z = self.encoder(x_masked)\n",
    "        return self.decoder(z), x_masked\n",
    "    \n",
    "    def apply_mask(self, x):\n",
    "        masked_x = x.clone()\n",
    "\n",
    "        for i in range(masked_x.size(0)):\n",
    "            ul_x = np.random.randint(0, x.size(2) - self.mask_size + 1)\n",
    "            ul_y = np.random.randint(0, x.size(3) - self.mask_size + 1)\n",
    "            masked_x[i, :, ul_x:ul_x+self.mask_size, ul_y:ul_y+self.mask_size] = 0\n",
    "\n",
    "        return masked_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpainting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pre-train the encoder using SVHN dataset\n",
    "inpainting_model = # TODO: Instantiate the colorization model\n",
    "inpainting_encoder = # TODO: Pre-train the encoder using SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pretrain_inpainting.py\n",
    "inpainting_model = InpaintingModel(mask_size=8)\n",
    "inpainting_encoder = train_ssl_model(inpainting_model,\n",
    "                                     svhn_train_loader, \n",
    "                                     svhn_test_loader, \n",
    "                                     criterion=nn.MSELoss(), \n",
    "                                     optimizer=optim.Adam(inpainting_model.parameters(), lr=0.001)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(inpainting_model, svhn_test_loader, device=device, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretext 3: Masked Autoencoder\n",
    "\n",
    "**Questions.** \n",
    "1. What are the values of `in_channels` and `out_channels` for the Masked Autoencoder model?\n",
    "2. Given the structure of the `train_ssl_model` function above, where should the masking of the original images happen?\n",
    "3. What loss should we use for the training?\n",
    "4. What optimizer would you choose for the training?\n",
    "5. Are there other hyper-parameters that we need to choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define the MAE model by adding an apply_mask method inside the class\n",
    "class MaskedAutoencoderModel(nn.Module):\n",
    "    # TODO: Implement the __init__ method\n",
    "\n",
    "    # TODO: Implement the forward method\n",
    "    \n",
    "    # TODO: Implement the apply_mask method\n",
    "    def apply_mask(self, x):\n",
    "        x_masked = x.clone()\n",
    "        mask = # TODO create a random mask with the right average number of pixels masked\n",
    "        x_masked[mask] = 0\n",
    "        return x_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mae_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pre-train the encoder using SVHN dataset\n",
    "mae_model = # TODO: Instantiate the colorization model\n",
    "mae_encoder = # TODO: Pre-train the encoder using SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pretrain_mae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(mae_model, svhn_test_loader, device=device, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task 1: Classification on Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(128 * 2 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        return self.fc(z)\n",
    "\n",
    "# Fine-tuning loop for classification\n",
    "def fine_tune_classification(encoder, train_loader, test_loader, epochs=5, encoder_in_channels=3):\n",
    "    model = Classifier(encoder).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Freeze the encoder's weights\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            if encoder_in_channels == 3:\n",
    "                images = torch.cat((images, images, images), dim=1)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if encoder_in_channels == 3:\n",
    "                images = torch.cat((images, images, images), dim=1)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fine-tune Colorization model\n",
    "fine_tune_classification(colorization_encoder, mnist_train_loader, mnist_test_loader, encoder_in_channels=1)\n",
    "\n",
    "# 2. Fine-tune Inpainting model\n",
    "fine_tune_classification(inpainting_encoder, mnist_train_loader, mnist_test_loader)\n",
    "\n",
    "# 3. Fine-tune Masked Autoencoder model\n",
    "fine_tune_classification(mae_encoder, mnist_train_loader, mnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task 2: Classification on SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(128 * 2 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        return self.fc(z)\n",
    "\n",
    "# Fine-tuning loop for classification\n",
    "def fine_tune_svhn(encoder, train_loader, test_loader, epochs=5, encoder_in_channels=3):\n",
    "    model = Classifier(encoder).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Freeze the encoder's weights\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            if encoder_in_channels == 1:\n",
    "                images = transforms.Grayscale()(images)  # Convert RGB to Grayscale\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if encoder_in_channels == 1:\n",
    "                images = transforms.Grayscale()(images)  # Convert RGB to Grayscale\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fine-tune Colorization model\n",
    "fine_tune_svhn(colorization_encoder, svhn_train_loader, svhn_test_loader, encoder_in_channels=1)\n",
    "\n",
    "# 2. Fine-tune Inpainting model\n",
    "fine_tune_svhn(inpainting_encoder, svhn_train_loader, svhn_test_loader)\n",
    "\n",
    "# 3. Fine-tune Masked Autoencoder model\n",
    "fine_tune_svhn(mae_encoder, svhn_train_loader, svhn_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
